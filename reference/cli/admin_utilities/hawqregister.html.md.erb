---
title: hawq register
---

Registers parquet-formatted table from an HDFS system into a corresponding table in HAWQ.

## Synopsis<a id="topic1__section2"></a>

``` pre
hawq register [-h hostname] [-p port] [-U username] <databasename> <tablename> <hdfspath> 

hawq register help
hawq register -? 

hawq register --version
```

## Prerequisites<a id="topic1__section3"></a>

The client machine where `hawq register` is executed must have the following:

-   Network access to and from all hosts in your HAWQ cluster (master and segments).
-   Network access to and from the hosts where the data to be loaded resides (ETL servers).
-   The files to be registered and the HAWQ table must be in the same HDFS cluster.
-   The target table must be configured with the correct data type mapping.

## Description<a id="topic1__section4"></a>

`hawq register` is a utility that loads and registers parquet data in HDFS into HAWQ, so that it can be directly accessed through HAWQ. Table data from the file or directory in the specified path is loaded into the appropriate table directory in HAWQ and the utility updates the corresponding metadata for the files. The table need not contain data.

Only parquet tables can be loaded using the `hawq register` command. Metadata for the parquet file(s) and the destination table must be consistent. Different  data types are used by HAWQ tables and parquet tables, so the data is mapped. You must verify that the structure of the parquet files and the HAWQ table are compatible before running `hawq register`. 

###Limitations for Registering Hive Tables to HAWQ
The currently-supported data types for generating Hive tables into HAWQ tables are: boolean, int, smallint, tinyint, bigint, float, double, string, binary, char, and varchar.     


## Options<a id="topic1__section5"></a>

<span class="tablecap">Table 1. General Options</span>

| Option| Description |
| ----  | ----- |
|-? (show help)   |Show help, then exit.|
|-\\\-version  | Show the version of this utility, then exit.|


<span class="tablecap">Table 2. Connection Options</span>

| Option| Description |
| ----  | ----- |
|-h *hostname* | Specifies the host name of the machine on which the HAWQ master database server is running. If not specified, reads from the environment variable `$PGHOST` or defaults to `localhost`.|
| -p *port*  |Specifies the TCP port on which the HAWQ master database server is listening for connections. If not specified, reads from the environment variable `$PGPORT` or defaults to 5432.|
|-U *username*  |The database role name to connect as. If not specified, reads from the environment variable `$PGUSER` or defaults to the current system user name.|
|*databasename*  |The database to register the parquet HDFS data into. |
|*tablename*  |The HAWQ table that will store the parquet data. The table cannot use hash distribution. |
|*hdfspath*|The path of the file or directory containing the files to be registered.|


## Examples<a id="topic1__section6"></a>

This example shows how to register a parquet file generated by Hive in HDFS having the path `hdfs://localhost:8020/temp/hive.paq`  into table `parquet_table` in HAWQ, which is in the database named `postgres`.

For the purposes of this example, assume that the location of the database is `hdfs://localhost:8020/hawq_default`, the tablespace id is 16385, the database id is 16387, the table filenode id is 77160, and the last file under the filenode is numbered 7.

Enter:

``` pre
$ hawq register postgres parquet_table hdfs://localhost:8020/temp/hive.paq
```

After running the `hawq register` command for the file location  `hdfs://localhost:8020/temp/hive.paq`, the corresponding new location of the file is:  `hdfs://localhost:8020/hawq_default/16385/16387/77160/8` in HDFS. The command then updates the metadata of the table `parquet_table` in HAWQ, which is contained in the table 'pg\_aoseg.pg\_paqseg\_77160'. The row-oriented table name prefix is pg_aoseg, and parquet table prefix is pg_paqseg, and 77160 is the relation id of the table.

To locate the table, you can either find the relation ID by looking up the catalog table pg_class by running `select oid from pg_class where relname=$relname` or by finding the table name by using the command `select segrelid from pg_appendonly where relid = $relid` then running `select relname from pg_class where oid = segrelid`.

##Data Type Mapping<a id="topic1__section7"></a>

HAWQ and parquet format tables do not use the same data types, so mapping must be used. Users should make sure their implementation is mapped to the appropriate data type before running `hawq register`.

<span class="tablecap">Table 3. HAWQ to Parquet Mapping</span>

|HAWQ Type   | Parquet Type  |
| :------------| :---------------|
| bool        | boolean       |
| int2/int4/date        | int32       |
| int8/money       | int64      |
| time/timestamptz/timestamp       | int64      |
| float4        | float       |
|float8        | double       |
|bit/varbit/bytea/numeric       | Byte array       |
|char/bpchar/varchar/name| Byte array |
| text/xml/interval/timetz  | Byte array  |
| macaddr/inet/cidr  | Byte array  |

**Additional HAWQ-to-Parquet Mapping**

**point**:  

``` 
group {
    required int x;
    required int y;
}
```

**circle:** 

```
group {
    required int x;
    required int y;
    required int r;
}
```

**box:**  

```
group {
    required int x1;
    required int y1;
    required int x2;
    required int y2;
}
```

**iseg:** 


```
group {
    required int x1;
    required int y1;
    required int x2;
    required int y2;
}
``` 

**path**:
  
```
group {
    repeated group {
        required int x;
        required int y;
    }
}
```

