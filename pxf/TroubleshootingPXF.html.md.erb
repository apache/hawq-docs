---
title: Troubleshooting PXF
---

## <a id="pxerrortbl"></a>PXF Errors

The following table lists some common errors encountered while using PXF:

<table>
<caption><span class="tablecap">Table 1. PXF Errors and Explanation</span></caption>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Error</th>
<th>Common Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ERROR:  invalid URI pxf://localhost:51200/demo/file1: missing options section</td>
<td><code class="ph codeph">LOCATION</code> does not include options after the file name: <code class="ph codeph">&lt;path&gt;?&lt;key&gt;=&lt;value&gt;&amp;&lt;key&gt;=&lt;value&gt;...</code></td>
</tr>
<tr class="even">
<td>ERROR:  protocol &quot;pxf&quot; does not exist</td>
<td>HAWQ is not compiled with PXF protocol. It requires the GPSQL version of HAWQ</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (0) from '&lt;x&gt;': There is no pxf servlet listening on the host and port specified in the external table url.</td>
<td>Wrong server or port, or the service is not started</td>
</tr>
<tr class="even">
<td>ERROR:  Missing FRAGMENTER option in the pxf uri: pxf://localhost:51200/demo/file1?a=a</td>
<td>No <code class="ph codeph">FRAGMENTER</code> option was specified in <code class="ph codeph">LOCATION</code>.</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':   type  Exception report   message   org.apache.hadoop.mapred.InvalidInputException:
<p>Input path does not exist: hdfs://0.0.0.0:8020/demo/file1  </p></td>
<td>File or pattern given in <code class="ph codeph">LOCATION</code> doesn't exist on specified path.</td>
</tr>
<tr class="even">
<td>ERROR: remote component error (500) from '&lt;x&gt;':   type  Exception report   message   org.apache.hadoop.mapred.InvalidInputException : Input Pattern hdfs://0.0.0.0:8020/demo/file* matches 0 files </td>
<td>File or pattern given in <code class="ph codeph">LOCATION</code> doesn't exist on specified path.</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;': PXF not correctly installed in CLASSPATH</td>
<td>Cannot find PXF Jar</td>
</tr>
<tr class="even">
<td>ERROR:  PXF API encountered a HTTP 404 error. Either the PXF service (tomcat) on data node was not started or PXF webapp was not started.</td>
<td>Either the required data node does not exist or PXF service (tcServer) on data node is not started or PXF webapp was not started</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':  type  Exception report   message   java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/client/HTableInterface</td>
<td>One of the classes required for running PXF or one of its plug-ins is missing. Check that all resources in the PXF classpath files exist on the cluster nodes</td>
</tr>
<tr class="even">
<td>ERROR: remote component error (500) from '&lt;x&gt;':   type  Exception report   message   java.io.IOException: Can't get Master Kerberos principal for use as renewer</td>
<td>Secure PXF: YARN isn't properly configured for secure (Kerberized) HDFS installs</td>
</tr>
<tr class="odd">
<td>ERROR: fail to get filesystem credential for uri hdfs://&lt;namenode&gt;:8020/</td>
<td>Secure PXF: Wrong HDFS host or port is not 8020 (this is a limitation that will be removed in the next release)</td>
</tr>
<tr class="even">
<td>ERROR: remote component error (413) from '&lt;x&gt;': HTTP status code is 413 but HTTP response string is empty</td>
<td>The PXF table number of attributes and their name sizes are too large for tcServer to accommodate in its request buffer. The solution is to increase the value of the maxHeaderCount and maxHttpHeaderSize parameters on server.xml on tcServer instance on all nodes and then restart PXF:
<p>&lt;Connector acceptCount=&quot;100&quot; connectionTimeout=&quot;20000&quot; executor=&quot;tomcatThreadPool&quot; maxKeepAliveRequests=&quot;15&quot;maxHeaderCount=&quot;&lt;some larger value&gt;&quot;maxHttpHeaderSize=&quot;&lt;some larger value in bytes&gt;&quot; port=&quot;${bio.http.port}&quot; protocol=&quot;org.apache.coyote.http11.Http11Protocol&quot; redirectPort=&quot;${bio.https.port}&quot;/&gt;</p></td>
</tr>
<tr class="odd">
<td>ERROR: remote component error (500) from '&lt;x&gt;': type Exception report message java.lang.Exception: Class com.pivotal.pxf.&lt;plugin name&gt; does not appear in classpath. Plugins provided by PXF must start with &quot;org.apache.hawq.pxf&quot;</td>
<td>Querying a PXF table that still uses the old package name (&quot;com.pivotal.pxf.*&quot;) results in an error message that recommends moving to the new package name (&quot;org.apache.hawq.pxf&quot;). </td>
</tr>
<tr class="even">
<td><strong>HBase Specific Errors</strong></td>
<td> </td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':   type  Exception report   message    org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for t1,,99999999999999 after 10 tries.</td>
<td>HBase service is down, probably HRegionServer</td>
</tr>
<tr class="even">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':  type  Exception report   message   org.apache.hadoop.hbase.TableNotFoundException: nosuch</td>
<td>HBase cannot find the requested table</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':  type  Exception report   message   java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/client/HTableInterface</td>
<td>PXF cannot find a required JAR file, probably HBase's</td>
</tr>
<tr class="even">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':   type  Exception report   message   java.lang.NoClassDefFoundError: org/apache/zookeeper/KeeperException</td>
<td>PXF cannot find ZooKeeper's JAR</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':  type  Exception report   message   java.lang.Exception: java.lang.IllegalArgumentException: Illegal HBase column name a, missing :</td>
<td>PXF table has an illegal field name. Each field name must correspond to an HBase column in the syntax &lt;column family&gt;:&lt;field name&gt;</td>
</tr>
<tr class="even">
<td>ERROR: remote component error (500) from '&lt;x&gt;': type Exception report message org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family a does not exist in region t1,,1405517248353.85f4977bfa88f4d54211cb8ac0f4e644. in table 't1', {NAME =&amp;gt; 'cf', DATA_BLOCK_ENCODING =&amp;gt; 'NONE', BLOOMFILTER =&amp;gt; 'ROW', REPLICATION_SCOPE =&amp;gt; '0', COMPRESSION =&amp;gt; 'NONE', VERSIONS =&amp;gt; '1', TTL =&amp;gt; '2147483647', MIN_VERSIONS =&amp;gt; '0', KEEP_DELETED_CELLS =&amp;gt; 'false', BLOCKSIZE =&amp;gt; '65536', ENCODE_ON_DISK =&amp;gt; 'true', IN_MEMORY =&amp;gt; 'false', BLOCKCACHE =&amp;gt; 'true'}</td>
<td>Required HBase table does not contain the requested column</td>
</tr>
<tr class="odd">
<td><strong>Hive-Specific Errors</td>
<td> </td>
</tr>
<tr class="even">
<td>ERROR:  remote component error (500) from '&lt;x&gt;':  type  Exception report   message   java.lang.RuntimeException: Failed to connect to Hive metastore: java.net.ConnectException: Connection refused</td>
<td>Hive Metastore service is down</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from '&lt;x&gt;': type  Exception report   message
<p>NoSuchObjectException(message:default.players table not found)</p></td>
<td>Table doesn't exist in Hive</td>
</tr>
<tr class="even">
<td><strong>JSON-Specific Errors</strong></td>
<td> </td>
</tr>
<tr class="odd">
<td>ERROR: No fields in record (seg0 slice1 host:&ltn&gt pid=&ltn&gt)
<p>DETAIL: External table &lttablename&gt</p></td>
<td>Check your JSON file for empty lines; remove them and try again</td>
</tr>
<tr class="odd">
<td>ERROR:  remote component error (500) from host:51200:  type  Exception report   message   &lttext&gt[0] is not an array node    description   The server encountered an internal error that prevented it from fulfilling this request.    exception   java.io.IOException: &lttext&gt[0] is not an array node (libchurl.c:878)  (seg4 host:40000 pid=&ltn&gt)  
<p>DETAIL:  External table &lttablename&gt</p></td>
<td>JSON field assumed to be an array, but it is a scalar field.
</td>
</tr>

</tbody>
</table>


## <a id="pxflogging"></a>PXF Logging
Enabling more verbose logging may aid PXF troubleshooting efforts.

PXF provides two categories of message logging - service-level and database-level.

### <a id="pxfsvclogmsg"></a>Service-Level Logging

PXF utilizes `log4j` for service-level logging. PXF-service-related log messages are captured in a log file specified by PXF's `log4j` properties file, `/etc/pxf/conf/pxf-log4j.properties`. The default PXF logging configuration will write `INFO` and more severe level logs to `/var/log/pxf/pxf-service.log`.

PXF provides more detailed logging when the `DEBUG` level is enabled.  To configure PXF `DEBUG` logging, uncomment the following line in `pxf-log4j.properties`:

``` shell
#log4j.logger.org.apache.hawq.pxf=DEBUG
```

and restart the PXF service:

``` shell
$ sudo service pxf-service restart
```

With `DEBUG` level logging now enabled, perform your PXF operations; for example, creating and querying an external table. (Make note of the time; this will direct you to the relevant log messages in `/var/log/pxf/pxf-service.log`.)

``` shell
$ psql
```

``` sql
gpadmin=# CREATE EXTERNAL TABLE hivetest(id int, newid int)
    LOCATION ('pxf://namenode:51200/pxf_hive1?PROFILE=Hive')
    FORMAT 'custom' (formatter='pxfwritable_import');
gpadmin=# select * from hivetest;
<select output>
```

Examine/collect the log messages from `pxf-service.log`.

**Note**: `DEBUG` logging is verbose and has a performance impact.  Remember to turn off PXF service `DEBUG` logging after you have collected the desired information.
 

### <a id="pxfdblogmsg"></a>Database-Level Logging

Enable HAWQ and PXF debug message logging during operations on PXF external tables by setting the `client_min_messages` server configuration parameter to `DEBUG2` in your `psql` session.

``` shell
$ psql
```

``` sql
gpadmin=# set client_min_messages=DEBUG2
gpadmin=# select * from hivetest;
...
DEBUG2:  churl http header: cell #19: X-GP-URL-HOST: localhost
DEBUG2:  churl http header: cell #20: X-GP-URL-PORT: 51200
DEBUG2:  churl http header: cell #21: X-GP-DATA-DIR: pxf_hive1
DEBUG2:  churl http header: cell #22: X-GP-profile: Hive
DEBUG2:  churl http header: cell #23: X-GP-URI: pxf://namenode:51200/pxf_hive1?profile=Hive
...
```

Examine/collect the log messages from `stdout`.

**Note**: `DEBUG2` database session logging has a performance impact.  Remember to turn off `DEBUG2` logging after you have collected the desired information.

``` sql
gpadmin=# set client_min_messages=NOTICE
```